{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Data Mining 2016 - Assignment 2: Know your Models\n",
    "\n",
    "In the first skills class, we mainly focussed on interpreting the data and forming hypotheses and expectations that rose from some natural intuition about the data. You examined and analysed the individual features (with histograms) and the combination of pairs of features (with scatter plots) to see to what extent these features separated the two classes. In this class we are diving into applying basic algorithms of classification to these problems to confirm (or reject) your own predictions.\n",
    "\n",
    "\n",
    "## 0.1 - Prepare your Data\n",
    "\n",
    "Because you have now developed some knowledge about the contents of the dataset we provided last time, it is best to use the same sets to capitalise on that. To really get a sense of the differences between several classification tasks, we advise that you again test for **more than one** dataset.\n",
    "\n",
    "## 0.2 - Refresher on the Models\n",
    "\n",
    "Today we will use $k$-nearest neighbours, as well as a decision tree classifier (J48) for classification.\n",
    "\n",
    "### $k$-NN\n",
    "Recall that every **feature vector** can be represented as a point in a graph. The basic algorithm behind $k$-NN just stores the points that you provide in the training phase. As such, the model has a sort of memory of the position of each point in its $N$-dimensional space (where $N$ is the amount of features, so 3 features could be represented in a 3-dimension space). Because it also knows the labels of these points (instances), it can predict new (to the algorithm unlabelled) instance in the test phase. Using a naive method for computation, this is done by measuring the distances between this new point, and every point that the model knows about. It then selects the top $k$ (therefore $k$-nearest) closest / nearest neighbours to this one point. For those interested, this distance calculation is done using [Euclidean Distances](https://en.wikipedia.org/wiki/Euclidean_distance) (for continuous variables at least). Basically it uses the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem) on the 'line' between the origin (0, ..., 0) and point 1, and the origin and point 2, so it calculates the unknown 'line' between point 1 and 2. In WEKA, we will be using the IBk implementation for $k$-NN.\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "J48 is an implementation of the C4.5 algorithm (which is an improvement on the ID3 algorithm) for constructing decision trees. It tries to determine certain values for each feature on which to make a decision (older than 50, is male), where preferably, the values that split the space most effectively between class labels will be highest in the tree. As you saw in the lecture, you can do this visually by trying to draw as few lines as possible to isolate the instances of only one label.\n",
    "\n",
    "## 1 - How to Evaluate\n",
    "\n",
    "Understanding how to score these algorithms on your classification task is an important part of checking if your hypotheses were correct and if it's actually doing what you want it to do.\n",
    "\n",
    "| nr | (true) label | prediction |\n",
    "| -- | ----- | ---------- |\n",
    "| 1  | dog   | cat        |\n",
    "| 2  | cat   | dog        |\n",
    "| 3  | ball  | ball       |\n",
    "| 4  | dog   | dog        |\n",
    "| 5  | cat   | cat        |\n",
    "| 6  | ball  | cat        |\n",
    "| 7  | dog   | dog        |\n",
    "| 8  | dog   | cat        |\n",
    "| 9  | dog   | cat        |\n",
    "| 10 | cat   | cat        |\n",
    "| 11 | cat   | cat        |\n",
    "| 12 | cat   | cat        |\n",
    "| 13 | cat   | cat        |\n",
    "| 14 | cat   | dog        |\n",
    "| 15 | ball  | ball       |\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Given the classification result above, compute the accuracy of the classifier.\n",
    "2. Make a confusion matrix of the results.\n",
    "3. Do the results make sense some way, if you look at the distribution of the labels?\n",
    "4. What would be a baseline score for this task?\n",
    "5. How well does the classifier do compared to the baseline?\n",
    "6. Do you think it has actually learned anything?\n",
    "7. Can you already come up with a reason why the classifier performs the way it does?\n",
    "\n",
    "---\n",
    "\n",
    "> Q: What's a baseline score again?\n",
    "\n",
    "> A: A baseline score is the most naive prediction you can come up with. You want to see if your classification algorithm actually learned anything, therefore predicting through methods that don't use any feature information is a way to form such a baseline. One option might for example be to just randomly predict a label for each of the instances. A harder baseline (especially for unbalanced data) is to just predict the majority class. We can see that the label 'cat' occurs a lot, so if we just predict 'cat' for everything we will probably already score pretty well. However, we learned nothing from the features at this point, they are stupid predictions. If your classifier is close, equal, or under this baseline, you might infer that it's not actually working.\n",
    "\n",
    "---\n",
    "\n",
    "## 2 - Actually Doing\n",
    "\n",
    "There is a huge range of factors that might affect the performance of your classifier. Some are inherent to the data, and hard to find a good solution for; others are due to your own design choices. It's very important to understand how certain classifier behave under different circumstances, and although there are some best practices that will be taught in these practicals, most of this you can only overcome empirically.\n",
    "\n",
    "- Select two or more classification problems (i.e. datasets).\n",
    "- Open up WEKA.\n",
    "\n",
    "In this part we will focus on running the two classifiers we discussed, and interpreting the results under different choices. Repeat these steps **for each different setting** in the questions below:\n",
    "\n",
    "- Load the data (resets any changes you made).\n",
    "- Make sure to remove any non-numeric / non-nominal features (i.e. showing text rather than a histogram in WEKA) (all models will be gray otherwise).\n",
    "- Do a Task (from the list below).\n",
    "- Click the `Classify` tab.\n",
    "- Click `use training set`.\n",
    "- Below the button `more options` select the feature you want to predict from the drop-down menu (i.e. make it the class label).\n",
    "- Run ZeroR (majority baseline) -- find this back under `Choose` (left top) `select -> rules -> ZeroR`.\n",
    "- Under `Choose` (left top) `select -> lazy -> IBK`, run.\n",
    "- Under `Choose` (left top) `select -> tree -> J48`, run.\n",
    "- Write down (copy) the results (accuracy and confusion matrix) for each of the three classifiers.\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "For each feature combination you determine the accuracy and the confusion matrix. Explain the results for yourself in terms of the nature of the feature you used.\n",
    "\n",
    "8. Remove all but the ONE feature you think is LEAST informative for the classification problem, **also keep the feature to predict**.\n",
    "9. Remove all but the TWO features you think are LEAST informative for the classification problem, **also keep the feature to predict**.\n",
    "10. Remove all but the ONE feature you think is MOST informative for the classification problem, **also keep the feature to predict**.\n",
    "11. Remove all but the TWO features you think are MOST informative for the classification problem, **also keep the feature to predict**.\n",
    "12. Run on all the features.\n",
    "\n",
    "Given that you've now ascertained the performance of these classifiers, compare your answers for all the settings and answer this:\n",
    "\n",
    "13. Why are there differences in the performance of both classifiers? Explain the answer in terms of decision boundaries.\n",
    "\n",
    "## 3 - Reporting Your Results\n",
    "\n",
    "So up until now, we've looked at data --- we've formed some hypotheses, and actually tested them by running the classifiers. As we've mentioned before, preparing, interpreting, and cleaning data is 80% of your work. Actual classification is a very small part. The most important part, however, is documenting and being able to report and motivate your process. This implies that _EVERY_ decision you are making along the way is part of your methodology; what features you selected, which parameters you changed, any preprocessing or filtering you did. Clearly listing and interpreting the performance of your classifiers and making sure that you judged them fairly is imperative.\n",
    "\n",
    "We've done quite the part of this already by writing down all results for the different classifiers. Note that in actual reports, we're not interested in reading what all individual classifiers scored. It's important to list some of the top performing results, but most important **why** you used a certain algorithm (i.e. you can't just write down that you tested 20 different ones and this one had the best score).\n",
    "\n",
    "This will be a recurring pattern for all following practicals; look at your data, --do new things--, accurately report your steps. We'll iteratively add parts you want to report on. For now, these will do:\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "14. To what extent did the classifier performance adhere to your expectations?\n",
    "15. J48 outputs a tree with decisions it came up with, did you find the features that you thought were most informative at the top level of this tree?\n",
    "16. How complex did the tree look for your specific task? Do you think this would be easily usable by a human?\n",
    "17. Why would you choose for either $k$-nn or the decision tree (other than their performance)?\n",
    "18. How well do you think your current classifier will generalize to new data? How do you know? Do you think you could improve this? If so, how?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
